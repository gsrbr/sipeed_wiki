{"/news/MaixPy3/run_lvgl/run_lvgl.html": {"title": "V831 如何使用 libmaix SDK C++ 开发", "content": "---\ntitle: V831 如何使用 libmaix SDK C++ 开发\nkeywords: V831, 交叉编译, lvgl\ndate: 2022-05-19\ndesc: V831 如何使用 libmaix SDK C++ 开发 交叉编译\ntags: V831，交叉编译\n---\n\n<!-- more -->\n\n作者[Song](QQ群友)，原文文件 [点我下载](https://dl.sipeed.com/fileList/others/wiki_news/v831_lvgl_news/220519UbuntuForV831%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.pptx)\n\n下面为重新整理的部分内容\n\n## 准备环境\n\n### 准备 linux\n\n一般在 linux 系统下开发比 windows 系统里问题少。因此首先自己整一个 linux 环境先。\n\n- 物理机和虚拟机都可以\n\n下面使用 Ubuntu18.04 作为实验系统。\n\n首先安装相关依赖 `sudo apt install build-essential cmake python3 sshpass git`\n\n然后确保一下cmake版本>=3.9\n![](./assets/cmake-version.png)\n\n### 配置交叉编译链\n\n先下载这个[点我跳转](https://dl.sipeed.com/shareURL/MaixII/MaixII-Dock/SDK/Toolchain)\n![](./assets/toolchain.png)\n\n接着再对应的下载目录执行下面的命令将工具链解压到 /opt 目录下\n`sudo tar -Jxvf toolchain-sunxi-musl-pack-2021-01-09.tar.xz -C /opt`\n\n### 获取libmaix源码\n\n新建一个文件夹后打开文件夹。\n在对应文件夹的终端使用下面命令来获取源码\n`git clone https://github.com/sipeed/libmaix --recursive`\n\n一定要确保全部下载完成，否则后面会因为找不到文件编译出错。\n\n### 开始编译\n\n#### 尝试编译helloworld\n\n先进入 libmaix 源码目录的 helloworld 文件夹里\n```bash\ncd libmaix/examples/hello-world\n```\n\n根据 CPU 架构选择工具链和前缀\n```bash\npython3 project.py --toolchain /opt/toolchain-sunxi-musl/toolchain/bin --toolchain-prefix arm-openwrt-linux-muslgnueabi- config\n```\n![](./assets/helloworld.png)\n\n接着就可以编译 helloworld 了。在上面的命令成功执行后接着执行下面的命令\n```bash\npython3 project.py menuconfig\n```\n\n若出现以下画面，则说明下载内容完整。若报错先尝试使用sudo执行，否则需重新下载解压\n![](./assets/helloworld-menuconfig.png)\n\n如第三项选择了Enable 3rd party component，则在编译时时间就会较长，因为会编译所有勾选的第三方组件。编译过程中可能会报一些warning，但最终出现下图画面则说明编译过程无误\n<img src=\"./assets/enable-3rd-party-component.png\">\n<img src=\"./assets/finish-helloworld.png\">\n\n前面的都顺利结束后在当前目录下会有一个 dist 文件夹。里面的 helloworld 文件就是 v831 的可执行程序。\n\n我们可以用 ssh 或者使用 v831在电脑上显示的U盘 把 helloworld 可执行文件传输到板子上\n\n接着在对应的目录下直接执行就可以看到结果了\n![](./assets/run-helloworld.png)\n\n#### lvgl编译与测试\n\n在 libmaix 源码的路径下的 /examples/mpp_v83x_vivo 执行下面命令后按照图示配置\n```bash\npython3 project.py menuconfig\n```\n<html>\n<div class=\"imbox\">\n    <img src=\"./assets/lvgl-1.png\" >\n    <img src=\"./assets/lvgl-2.png\" >\n</div>\n</html>\n\n检查选项是否如以上配置所示，确认无误后在命令行执行\n```bash\npython3 project.py build\n```\n- 注：若同时勾选所有组件，则可能会发生重复定义函数的报错导致编译失败\n\n若出现如图所示情况，则说明编译成功\n<img src=\"./assets/lvgl-3.png\">\n\n在板子上运行\n<html>\n<div class=\"imbox\">\n    <img src=\"./assets/lvgl-4.png\" height=300>\n    <img src=\"./assets/lvgl-5.png\" height=300>\n<style>\n.imbox{\n     display:flex;\n     flex-direction: row;\n     }\n</style>\n</div>\n</html>"}, "/news/MaixPy3/difference.html": {"title": "MaixPy 与 MaixPy3 的区别", "content": "---\ntitle: MaixPy 与 MaixPy3 的区别\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: MaixPy 与 MaixPy3 的区别\ndate: 2022-03-07\ntags: MaixPy,MaixPy3\n---\n\n<!-- more -->\n\n## 区别是？\n\n因为使用 MaixPy 的同学可能有两类人群，一类是从 MicroPython 一路使用过来的，另一类是从 Python3 过来的，所以针对两边的差异，分别做一下说明。\n\n可以这样理解，它们都是专门为 AIoT 提供的 Python 开发环境，提供了各种各样的模块。\n\n- MaixPy 指的是基于 MicroPython 的环境制作的。\n\n- MaixPy3 指的是基于 Linux Python3 的环境制作的。\n\n> 前者是基于 MCU 无系统的，后者是基于 Linux 系统。\n\n除了基本的 Python3 语法一致，在提供的模块方面的存在着不小的差异。\n\n### Python3 与 MicroPython 的区别\n\n大多数时候，Python 的发展以 Python3 为主，以下列出一些与 Python3 的差异化信息。\n\n- MicroPython 和 Python3 在 Python 语法上保持高度的一致性，常用的标准语法命令都已经支持。\n\n- MicroPython 虽然只实现了 Python3 的标准库和容器库的一些部分，常见容器库有同类功能，但不同名的模块，但大多算法类的 Python 逻辑代码是可以拿来即用的。\n\n- MicroPython 兼容实现的 Python3 的异常机制、没有实现元类（metaclass）机制，独立的 GC 机制。\n\n- 在许当不同的硬件微芯片（最低在 nRF51）的移植上， MicroPython 代码接口缺乏一致性，呈现碎片化。\n\n- MicroPython 编译（mpy-corss）后得到的是 mpy ，而不是 Python3 的 pyc 文件。\n\n- MicroPython 在移植 Python3 代码时，经常缺少各种方法，所以要习惯寻找同类接口，而它们的使用方法除了看文档外就只能看源码。\n\n### 总结\n\n- MaixPy 相比 MaixPy3 功能要更简单（简陋）。\n- MaixPy 和 MaixPy3 的开发工具不同。\n- MaixPy 标准库（MicroPython）相比 MaixPy3 有一定的不足。\n- MaixPy 的外设驱动模块具体函数存在差异。\n- 不同的芯片执行效率有差异，MaixPy 和 MaixPy3 的有着不同的内存与性能消耗。\n\n> 如有更多欢迎补充。"}, "/news/MaixPy3/v831_Distance/v831_Distance.html": {"title": "V831完美的单目测距", "content": "---\ntitle: V831完美的单目测距\nkeywords: V831, 单目, 测距\ndate: 2022-03-28\ndesc: V831完美的单目测距\ntags: V83x,单目测距\n---\n\n<!-- more -->\n\n作者[我与nano](https://qichenxi.blog.csdn.net/?type=blog)，[原文链接](https://blog.csdn.net/qq_51963216/article/details/123745657)\n\n## 前言\n\n经过一下午的努力，最终终于实现了完美的单目测距，网上教的都是opencv怎么测算距离，人家有函数唉，入手了V831，做了人脸识别，同时进行了测距，K210通用。废话不多说上图。\n\n![单目测距](./assets/distance_measure.png)\n![摄像头距离](./assets/Camera_length.png)\n它那个镜头其实还要在靠近里面一点，距离应该是28.4到28.5之间。测得真的特别准。\n\n## 单目测距的原理\n![principle](./assets/principle.png)\n\n小孔成像。很简单，用的是小孔成像，原理大家都知道。该怎么做呢。\n我们需要以下几个参数：\n1、相机焦距\n2、物体宽度\n3、一个常数\n\n## 参数计算\n\n### 相机焦距\n假设我们有一个宽度为 W 的目标。然后我们将这个目标放在距离我们的相机为 D 的位置。我们用相机对物体进行拍照并且测量物体的像素宽度 P 。这样我们就得出了相机焦距的公式：\n\nF = (P x D) / W\n\n举个例子，假设我在离相机距离 D = 28cm的地方放一张 待识别图片（W = 13)并且拍下一张照片。我测量出照片的像素宽度为 P = 53 像素\n\n![](./assets/length_calculate.png)\n\n因此我的焦距 F 是：\n\nF = (53*28) / 13 = 116\n\n有人会问像素怎么获得呢，直接看代码吧\n```python\n img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n            img.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n            img.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\n            img.draw_string(0,30, \"x=\"+str(((box[0]+box[3])/2)-35), color= font_color)\n            img.draw_string(70,30, \"y=\"+str((box[1]+box[2])/2), color= font_color)\n\n            Lm = (box[1]+box[3])/2\n            length = K*13/Lm\n            img.draw_string(0,60 , \"Z=\"+str(length), color= font_color)\n\n```\n你识别到一个物体，然后给它画框，用一个列表表示出来四个点\nLm=（box[1]+box[3]）/2 这个就是像素值\n\n### 测距\n继续将相机移动，靠近或者离远物体或者目标时，可以用相似三角形计算出物体离相机的距离：\nL= (W x F) / P\n假设我将相机移到距离目标 28cm 的地方识别物体。通过自动的图形处理我可以获得图片的像素为 53像素。将这个代入公式，得：\nL= (13 x 116) / 53 = 28\n这样我们就精准的算出了距离。\n\n附上代码\n```python\nfrom maix import camera, image, display\nimport serial\nser = serial.Serial(\"/dev/ttyS1\",115200)    # 连接串口\nK=116\nclass Face_recognize :\n    score_threshold = 70                            #识别分数阈值\n    input_size = (224, 224, 3)                      #输入图片尺寸\n    input_size_fe = (128, 128, 3)                   #输入人脸数据\n    feature_len = 256                               #人脸数据宽度\n    steps = [8, 16, 32]                             #\n    channel_num = 0                                 #通道数量\n    users = []                                      #初始化用户列表\n    threshold = 0.5                                         #人脸阈值\n    nms = 0.3\n    max_face_num = 3                                        #输出的画面中的人脸\n    def __init__(self):\n        from maix import nn, camera, image, display\n        from maix.nn.app.face import FaceRecognize\n        for i in range(len(self.steps)):\n            self.channel_num += self.input_size[1] / self.steps[i] * (self.input_size[0] / self.steps[i]) * 2\n        self.channel_num = int(self.channel_num)     #统计通道数量\nglobal face_recognizer\nface_recognizer = Face_recognize()\nwhile True:\n    img = camera.capture()                       #获取224*224*3的图像数据\n    AI_img = img.copy().resize(224, 224)\n    faces = face_recognizer.face_recognizer.get_faces(AI_img.tobytes(),False)           #提取人脸特征信息\n\n    if faces:\n        for prob, box, landmarks, feature in faces:\n            disp_str = \"face\"\n            bg_color = (0, 255, 0)\n            font_color=(255, 0, 0)\n            box,points = face_recognizer.map_face(box,landmarks)\n            font_wh = image.get_string_size(disp_str)\n            for p in points:\n                img.draw_rectangle(p[0] - 1, p[1] -1, p[0] + 1, p[1] + 1, color=bg_color)\n            img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n            img.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n            img.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\n            img.draw_string(0,30, \"x=\"+str(((box[0]+box[3])/2-28)), color= font_color)\n            img.draw_string(70,30, \"y=\"+str((box[1]+box[2])/2-20), color= font_color)\n            x=(box[0]+box[3])/2-28\n            y=(box[1]+box[2])/2\n            Lm = (box[1]+box[3])/2\n            length = K*13/Lm\n            img.draw_string(0,60 , \"Z=\"+str(round(length)), color= font_color)\n           \n    display.show(img)\n\n```\n\n## 总结\n\n**主要原理就是小孔成像**"}, "/news/MaixPy3/maixpy3_easyuse/maixpy3_easyuse.html": {"title": "MaixPy3 源码怎么样", "content": "---\ntitle: MaixPy3 源码怎么样\nkeywords: V831, Maixpy3\ndate: 2022-04-29\ntags: MaixPy3,QQ\n---\n\n这里只使用一张图来说明一下相关的结论\n\n<!-- more -->\n\n![](./assets/pic.jpg)"}, "/news/MaixPy3/key_face_recognize.html": {"title": "V831的人脸识别", "content": "---\ntitle: V831的人脸识别\nkeywords: MaixII-Dock, MaixPy3, 人脸识别, V831\ndesc: V831的人脸识别\ndate: 2022-03-15\ntags: MaixII-Dock, MaixPy3\n---\n\n在文档中看到 V831 可以用来实现人脸识别，于是就将按键也添加到人脸识别中。\n\n<!-- more -->\n\n实现一个可以通过按键进行控制的人脸识别，进行人脸信息的添加和删除控制\n## 源码\n\n```python\nfrom maix import nn, camera, image, display\nfrom maix.nn.app.face import FaceRecognize\nimport time\nfrom evdev import InputDevice\nfrom select import select\n\n\nscore_threshold = 70                            #识别分数阈值\ninput_size = (224, 224, 3)                      #输入图片尺寸\ninput_size_fe = (128, 128, 3)                   #输入人脸数据\nfeature_len = 256                               #人脸数据宽度\nsteps = [8, 16, 32]                             #\nchannel_num = 0                                 #通道数量\nusers = []                                      #初始化用户列表\nnames = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]  #人脸标签定义\nmodel = {                                                                                                                                   \n    \"param\": \"/home/model/face_recognize/model_int8.param\",\n    \"bin\": \"/home/model/face_recognize/model_int8.bin\"\n}\nmodel_fe = {\n    \"param\": \"/home/model/face_recognize/fe_res18_117.param\",\n    \"bin\": \"/home/model/face_recognize/fe_res18_117.bin\"\n}\n\n\nfor i in range(len(steps)):\n    channel_num += input_size[1] / steps[i] * (input_size[0] / steps[i]) * 2\nchannel_num = int(channel_num)     #统计通道数量\noptions = {                             #准备人脸输出参数\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": input_size\n    },\n    \"outputs\": {\n        \"output0\": (1, 4, channel_num) ,\n        \"431\": (1, 2, channel_num) ,\n        \"output2\": (1, 10, channel_num) \n    },\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.0078125, 0.0078125, 0.0078125],\n}\noptions_fe = {                             #准备特征提取参数\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"inputs_blob\": input_size_fe\n    },\n    \"outputs\": {\n        \"FC_blob\": (1, 1, feature_len)\n    },\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.0078125, 0.0078125, 0.0078125],\n}\nkeys = InputDevice('/dev/input/event0')\n\nthreshold = 0.5                                         #人脸阈值\nnms = 0.3                                               \nmax_face_num = 1                                        #输出的画面中的人脸的最大个数\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\nprint(\"-- load model:\", model_fe)\nm_fe = nn.load(model_fe, opt=options_fe)\nprint(\"-- load ok\")\nface_recognizer = FaceRecognize(m, m_fe, feature_len, input_size, threshold, nms, max_face_num)\n\ndef get_key():                                      #按键检测函数\n    r,w,x = select([keys], [], [],0)\n    if r:\n        for event in keys.read(): \n            if event.value == 1 and event.code == 0x02:     # 右键\n                return 1\n            elif event.value == 1 and event.code == 0x03:   # 左键\n                return 2\n            elif event.value == 2 and event.code == 0x03:   # 左键连按\n                return 3\n    return 0\n\ndef map_face(box,points):                           #将224*224空间的位置转换到240*240或320*240空间内\n    # print(box,points)\n    if display.width() == display.height():\n        def tran(x):\n            return int(x/224*display.width())\n        box = list(map(tran, box))\n        def tran_p(p):\n            return list(map(tran, p))\n        points = list(map(tran_p, points))\n    else:\n        # 168x224(320x240) > 224x224(240x240) > 320x240\n        s = (224*display.height()/display.width()) # 168x224\n        w, h, c = display.width()/224, display.height()/224, 224/s\n        t, d = c*h, (224 - s) // 2 # d = 224 - s // 2 == 28\n        box[0], box[1], box[2], box[3] = int(box[0]*w), int((box[1]-28)*t), int(box[2]*w), int((box[3])*t)\n        def tran_p(p):\n            return [int(p[0]*w), int((p[1]-d)*t)] # 224 - 168 / 2 = 28 so 168 / (old_h - 28) = 240 / new_h\n        points = list(map(tran_p, points))\n    # print(box,points)\n    return box,points\n\ndef darw_info(draw, box, points, disp_str, bg_color=(255, 0, 0), font_color=(255, 255, 255)):    #画框函数\n    box,points = map_face(box,points)\n    font_wh = image.get_string_size(disp_str)\n    for p in points:\n        draw.draw_rectangle(p[0] - 1, p[1] -1, p[0] + 1, p[1] + 1, color=bg_color)\n    draw.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n    draw.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n    draw.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\ndef recognize(feature):                                                                   #进行人脸匹配\n    def _compare(user):                                                         #定义映射函数\n        return face_recognizer.compare(user, feature)                      #推测匹配分数 score相关分数\n    face_score_l = list(map(_compare,users))                               #映射特征数据在记录中的比对分数\n    return max(enumerate(face_score_l), key=lambda x: x[-1])                #提取出人脸分数最大值和最大值所在的位置\n\ndef run():\n    img = camera.capture()                       #获取224*224*3的图像数据\n    AI_img = img.copy().resize(224, 224)\n    if not img:\n        time.sleep(0.02)\n        return\n    faces = face_recognizer.get_faces(AI_img.tobytes(),False)           #提取人脸特征信息\n    if faces:\n        for prob, box, landmarks, feature in faces:\n            key_val = get_key()\n            if key_val == 1:                                # 右键添加人脸记录\n                if len(users) < len(names):\n                    print(\"add user:\", len(users))\n                    users.append(feature)\n                else:\n                    print(\"user full\")\n            elif key_val == 2:                              # 左键删除人脸记录\n                if len(users) > 0:\n                    print(\"remove user:\", names[len(users) - 1])\n                    users.pop()\n                else:\n                    print(\"user empty\")\n\n            if len(users):                             #判断是否记录人脸\n                maxIndex = recognize(feature)\n\n                if maxIndex[1] > score_threshold:                                      #判断人脸识别阈值,当分数大于阈值时认为是同一张脸,当分数小于阈值时认为是相似脸\n                    darw_info(img, box, landmarks, \"{}:{:.2f}\".format(names[maxIndex[0]], maxIndex[1]), font_color=(0, 0, 255, 255), bg_color=(0, 255, 0, 255))\n                    print(\"user: {}, score: {:.2f}\".format(names[maxIndex[0]], maxIndex[1]))\n                else:\n                    darw_info(img, box, landmarks, \"{}:{:.2f}\".format(names[maxIndex[0]], maxIndex[1]), font_color=(255, 255, 255, 255), bg_color=(255, 0, 0, 255))\n                    print(\"maybe user: {}, score: {:.2f}\".format(names[maxIndex[0]], maxIndex[1]))\n            else:                                           #没有记录脸\n                darw_info(img, box, landmarks, \"error face\", font_color=(255, 255, 255, 255), bg_color=(255, 0, 0, 255))\n\n\n    display.show(img)\n\n\n\nif __name__ == \"__main__\":\n    import signal\n    def handle_signal_z(signum,frame):\n        print(\"APP OVER\")\n        exit(0)\n    signal.signal(signal.SIGINT,handle_signal_z)\n    while True:\n        run()\n\n```"}, "/news/MaixPy3/camera_resize/camera_resize.html": {"title": "MaixPy3 Image.resize 的效果", "content": "---\ntitle: MaixPy3 Image.resize 的效果\nkeywords: MaixPy3,\ndesc: 这里展示一下 MaixPy3 Image.resize 的效果\ndate: 2022-07-01\ntags: MaixPy3, resize, teedoc\ncover: ./assets/resize.png\n---\n\n点击下载对应的 ipynb 文件后导入到 jupyter 照样能阅读 [源文件](https://dl.sipeed.com/fileList/others/wiki_news/maixpy3_resize/camera_resize.ipynb)\n\n<!-- more -->\n\n## 先写一些 jupyter 用法\n\n- 每一个框框都被称之为单元格\n\n- 单元格左方会有 蓝色 或者 绿色 两种颜色。绿色表示编辑模式；蓝色表示命令模式。\n    - 通用：\n        - Shift+ Enter ：运行单元格，且以命令模式切换到下一个单元格\n        - Ctrl + Enter ：运行单元格，且进入命令模式\n    - 编辑模式中：\n        - Esc       ：进入命令模式\n    - 命令模式中：\n        - **h    :打开帮助**\n        - Enter :进入编辑模式\n        - x    :剪切单元格\n        - c    :复制单元格\n        - v    :粘贴单元格\n        - dd   :删除整个单元格\n        - ii   :终止运行 \n\n## 开始演示效果\n\n### 试一下推流\n\n```python\nimport os\nos.system(\"killall python3\")             #先杀一次预防 camera is busy\nfrom maix import camera, display, image #引入python模块包\nwhile True:\n    img = camera.capture()    #从摄像头中获取一张图像\n    display.show(img)         #将图像显示出来\n    \n    #因为这是死循环，所以按一下 Esc 进入编辑模式然后 ii 终止一下代码\n```\n\n效果如下（视频被截成图片了）：\n\n![推流](./assets/forever_show.jpeg)\n\n### 设置一下图像分辨率\n\n下面就直接捕获原图和使用 image.resize() 放一起对比一下\n\n#### 试试 240*240 的显示效果\n\n``` python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(240, 240))   #设置获取图像分辨率\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"240x240.jpg\")\n```\n\n![240*240](./assets/240_240.jpeg)\n\n#### 240\\*240图片resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(240, 240))\nimg = camera.capture().resize(224, 224)\ndisplay.show(img)\n```\n\n![240*240->224*224](./assets/240_240_224_224.png)\n\n#### 试试 320*240 的显示效果\n\n```python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(320, 240))\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"320x240.jpg\")\n```\n\n![320*240](./assets/320_240.jpeg)\n\n#### 320\\*240图像resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(320, 240))\nimg = camera.capture().resize(224, 244)\ndisplay.show(img)\n```\n\n![320*240->224*224](./assets/320_240_224_224.png)\n\n#### 再试试 320*180 显示效果\n\n```python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(320, 180))    #设置摄像头分辨率\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"320x180.jpg\")\n```\n\n![320*180](./assets/320_180.jpeg)\n\n#### 320\\*180图像resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(320, 180))\nimg = camera.capture().resize(224, 244)\ndisplay.show(img)\n```\n\n![320*180->224*224](./assets/320_180_224_224.png)"}, "/news/Lichee/RV/D1-ncnn/D1_ncnn_new.html": {"title": "又在全志d1开发板上玩ncnn", "content": "---\ntitle: 又在全志d1开发板上玩ncnn\nkeywords: D1, RV, Lichee, ncnn, \ndesc: 又在全志d1开发板上玩ncnn\ndate: 2022-03-28\ntags: RV, ncnn\n---\n\n\n<!-- more -->\n\n转载自知乎用户 [nihui](https://www.zhihu.com/people/nihui-2) [原文链接](https://zhuanlan.zhihu.com/p/441176926)，原文写于 2021-07-03\n\n又在全志d1开发板上玩ncnn\n\n**可在不修改本文章内容和banner图前提下，转载本文**\n\n## 0x0 工具链变得更好了\n距上次[在全志d1开发板上玩ncnn](./D1_ncnn.html)，已经过去了5个月\n\n在此期间，ncnn收到perfxlab和腾讯犀牛鸟开源人才的学生有关riscv vector的优化\n\n但更重要的是，平头哥收到了社区的反馈，提供了新版工具链\n\n- 支持了 risc-v vector intrinsic v1.0\n- 修复了 release 模式编译 ncnn 时的非法指令问题\nhttps://occ.t-head.cn/community/download?id=3987221940543754240\n\n旧版本工具链的 gcc 比较笨，经常做些负优化，于是试试全新的工具链\n\n## 0x1 配置新的 cmake toolchain\n```bash\n旧\n-march=rv64gcvxtheadc -mabi=lp64d -mtune=c906 -DRVV_SPEC_0_7 -D__riscv_zfh=1 -static\n\n新\n-march=rv64gcv0p7_zfh_xtheadc -mabi=lp64d -mtune=c906 -static\n```\n- arch 参数要用 v0p7，不能用默认的 v，否则会生成非法指令\n- 删除 -DRVV_SPEC_0_7，开启 ncnn 的 rvv-1.0 intrinsic 代码\n- 删除 -D__riscv_zfh=1，arch 参数的 zfh 中已经指代\n\n放在 ncnn/toolchains/c906-v222.toolchain.cmake\n\n## 0x2 工具链修复\n\n因为 rvv-0.7 缺少某些指令支持，遇到一些 rvv-1.0 的代码会生成 unknown op\n```bash\nfneg\nfrec7\nfrsqrt7\n```\n因此要修改下工具链头文件\n\n打开 Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.2.2/lib/gcc/riscv64-unknown-linux-gnu/10.2.0/include/riscv_vector.h\n\n- 找到以下三行\n```h\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rec7)\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rsqrt7)\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, neg)\n```\n- 注释掉\n\n```h\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rec7)\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rsqrt7)\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, neg)\n```\n\n- 找到文件末尾的三个 #endif，添加以下兼容代码，保存\n\n```h\n#endif\n\n#define vfneg_v_f32m1(x, vl) vfsgnjn_vv_f32m1(x, x, vl)\n#define vfneg_v_f32m2(x, vl) vfsgnjn_vv_f32m2(x, x, vl)\n#define vfneg_v_f32m4(x, vl) vfsgnjn_vv_f32m4(x, x, vl)\n#define vfneg_v_f32m8(x, vl) vfsgnjn_vv_f32m8(x, x, vl)\n#define vfneg_v_f16m1(x, vl) vfsgnjn_vv_f16m1(x, x, vl)\n#define vfneg_v_f16m2(x, vl) vfsgnjn_vv_f16m2(x, x, vl)\n#define vfneg_v_f16m4(x, vl) vfsgnjn_vv_f16m4(x, x, vl)\n#define vfneg_v_f16m8(x, vl) vfsgnjn_vv_f16m8(x, x, vl)\n\n#define vfrec7_v_f32m1(x, vl) vfrdiv_vf_f32m1(x, 1.f, vl)\n#define vfrec7_v_f32m2(x, vl) vfrdiv_vf_f32m2(x, 1.f, vl)\n#define vfrec7_v_f32m4(x, vl) vfrdiv_vf_f32m4(x, 1.f, vl)\n#define vfrec7_v_f32m8(x, vl) vfrdiv_vf_f32m8(x, 1.f, vl)\n#define vfrec7_v_f16m1(x, vl) vfrdiv_vf_f16m1(x, 1.f, vl)\n#define vfrec7_v_f16m2(x, vl) vfrdiv_vf_f16m2(x, 1.f, vl)\n#define vfrec7_v_f16m4(x, vl) vfrdiv_vf_f16m4(x, 1.f, vl)\n#define vfrec7_v_f16m8(x, vl) vfrdiv_vf_f16m8(x, 1.f, vl)\n\n#define vfrsqrt7_v_f32m1(x, vl) vfrdiv_vf_f32m1(vfsqrt_v_f32m1(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m2(x, vl) vfrdiv_vf_f32m2(vfsqrt_v_f32m2(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m4(x, vl) vfrdiv_vf_f32m4(vfsqrt_v_f32m4(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m8(x, vl) vfrdiv_vf_f32m8(vfsqrt_v_f32m8(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m1(x, vl) vfrdiv_vf_f16m1(vfsqrt_v_f16m1(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m2(x, vl) vfrdiv_vf_f16m2(vfsqrt_v_f16m2(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m4(x, vl) vfrdiv_vf_f16m4(vfsqrt_v_f16m4(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m8(x, vl) vfrdiv_vf_f16m8(vfsqrt_v_f16m8(x, vl), 1.f, vl)\n\n#endif\n#endif\n```\n\n## 下载和编译ncnn\n这次可以用 release 编译啦！\n```bash\ngit clone https://github.com/Tencent/ncnn.git\ncd ncnn\nmkdir build-c906\ncd build-c906\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchains/c906-v222.toolchain.cmake -DCMAKE_BUILD_TYPE=release -DNCNN_OPENMP=OFF -DNCNN_THREADS=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_RVV=ON -DNCNN_SIMPLEOCV=ON -DNCNN_BUILD_EXAMPLES=ON ..\nmake -j32\n```\n\n## 新旧工具链的性能测试对比\n![](./assets/ncnn_new/ncnn_new_001.jpg)\n\n![](./assets/ncnn_new/ncnn_new_002.jpg)\n\n## 0x5 欢迎关注 ncnn github，加qq群交流！\nhttps://github.com/Tencent/ncnn\nqq群在 ncnn github 首页 readme 中～"}, "/news/Lichee/RV/D1-ncnn/D1_ncnn.html": {"title": "在全志d1开发板上玩ncnn", "content": "---\ntitle: 在全志d1开发板上玩ncnn\nkeywords: D1, RV, Lichee, ncnn, \ndesc: 在全志d1开发板上玩ncnn\ndate: 2022-03-28\ntags: RV, ncnn\n---\n\n<!-- more -->\n\n转载自知乎用户 [nihui](https://www.zhihu.com/people/nihui-2) [原文链接](https://zhuanlan.zhihu.com/p/386312071)，原文写于 2021-07-03\n\n在全志d1开发板上玩ncnn\n\n**可在不修改本文章内容和banner图前提下，转载本文**\n```\n这是我最后一次优化 risc-v\n这 1.4w 行代码是我最后的倔强\n你们不可能再看见我为这个 d1 写一行代码，不可能\n这 96 个 cpp 文件，我要用到 2030 年\n```\n**首先感谢全志科技公司送了我d1开发板，以及sipeed、rvboards在系统底层技术工作和支持，才有了ncnn AI推理库在risc-v架构上更好的优化 qwqwqwq**\n\n## 0x0 ncnn risc-v 优化情况\n[ncnn](https://github.com/Tencent/ncnn) 是腾讯开源的神经网络推理框架\n- 支持深度学习模型 caffe/mxnet/keras/pytorch(onnx)/darknet/tensorflow(mlir)\n- 跨平台：Windows/Linux/MacOS/Android/iOS/WebAssembly/...\n- 兼容多种 CPU 架构：x86/arm/mips/risc-v/...\n- 支持 GPU 加速：NVIDIA/AMD/Intel/Apple/ARM-Mali/Adreno/...\n- 支持各种常见的模型结构，比如 mobilenet/shufflenet/resnet/LSTM/SSD/yolo...\n- 很强，qq群请移驾 ncnn github 首页README\n_因为据全（某）志（人）说，全志的用户基础都挺一般，可能不知道 ncnn 是什么东西，所以便罗嗦一番..._\n\n从上次发了开箱自拍jpg，到现在一个月了，ncnn risc-v vector 优化情况还算不错，大部分重要的优化都做了，剩下一些会留给社区学生pr，和慢慢变聪明的编译器\n\nncnn risc-v 目前使用 rvv-1.0 intrinisc 编写优化代码，并支持任意 vlen 的配置，面向未来顺便兼容了 d1开发板\n\n- rvv-0.7.1 某些 intrinisc 转换可能有效率问题\n- 有遇到过 0.7.1 intrinisc 行为怪异只能写 C 代码绕过\n- gcc 还比较笨，每行 intrinisc 都会加一条无用的 setvli 指令\n- 因为没法同时兼容 rvv-1.0 和 rvv-0.7.1，便没有写汇编\n- 一些算子，如 hardswish/hardsigmoid/binaryop/eltwise/slice/... 待优化（欢迎pr！！！qaq）\n下面这张表只是最近一周多的情况。如果跟最开始比，柱状图就太高了...\n![](./assets/ncnn/001.jpg)\n\n![](./assets/ncnn/002.jpg)\n\n## 0x1 准备交叉编译工具链\n去平头哥芯片开放社区下载 工具链-900 系列 \nhttps://occ.t-head.cn/community/download?id=3913221581316624384\n比如 riscv64-linux-x86_64-20210512.tar.gz，下载后解压缩，设置环境变量\n```bash\ntar -xf riscv64-linux-x86_64-20210512.tar.gz\nexport RISCV_ROOT_PATH=/home/nihui/osd/riscv64-linux-x86_64-20210512\n```\n## 0x2 下载和编译ncnn\n\n为 d1 架构交叉编译 ncnn\n\n**因为编译器 bug，release 编译会导致运行时非法指令错误，必须使用 relwithdebinfo 编译哦**\n\n- ncnn 已支持直接用 simpleocv 替代 opencv 编译出 examples\n- **不需要配opencv啦！**\n- **不需要配opencv啦！**\n- **不需要配opencv啦！（重要，说了三遍）**\n\n```bash\ngit clone https://github.com/Tencent/ncnn.git\ncd ncnn\nmkdir build-c906\ncd build-c906\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchains/c906.toolchain.cmake -DCMAKE_BUILD_TYPE=relwithdebinfo -DNCNN_OPENMP=OFF -DNCNN_THREADS=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_RVV=ON -DNCNN_SIMPLEOCV=ON -DNCNN_BUILD_EXAMPLES=ON ..\nmake -j32\n```\n## 0x3 测试benchncnn\n**d1 默认的 TinaLinux 执行 ncnn 程序时会发生非法指令错误，必须使用 Debian 系统哦**\n- vgg16 这类大型模型在内存不足时会发生 segmentation fault，是 d1开发板硬件条件不够，不管即可\n  \n将 `ncnn/build-c906/benchmark/benchncnn` 和 `ncnn/benchmark/*.param` 拷贝到 d1开发板上\n```bash\n./benchncnn 4 1 0 -1 0\n```\n\n## 0x4 测试example\n将 `ncnn/build-c906/examples/nanodet` 和测试图片拷贝到 d1开发板上\n从这里下载 nanodet 模型文件并拷贝到 d1开发板上\nhttps://github.com/nihui/ncnn-assets/tree/master/models\n```bash\n./nanodet test.jpg\n```\n输出检测结果信息，并保存在 image.png\n```\n0 = 0.82324 at 200.04 44.89 198.96 x 253.33\n0 = 0.78271 at 32.98 63.45 178.15 x 232.92\n56 = 0.45923 at 1.46 71.92 90.14 x 117.85\nimshow save image to image.png\nwaitKey stub\n```\n把image.png下载到本地查看，结果已经画在图片上了！d1开发板AI目标检测成功 w\n![](./assets/ncnn/003.jpg)\n\n## 0x5 mips大概也会安排啦，欢迎关注 ncnn github，加qq群交流！\nhttps://github.com/Tencent/ncnn\n\nqq群在 ncnn github 首页 readme 中～"}, "/news/Lichee/RV/D1_RTL8723DS_Drivers/D1_RTL8723DS_Drivers.html": {"title": "D1 LicheeRV Dock 移植RTL8723DS驱动", "content": "---\ntitle: D1 LicheeRV Dock 移植RTL8723DS驱动\nkeywords: D1, RTL8723DS, 驱动\ndesc: RTL8723DS驱动移植\ndate: 2022-04-02\ntags: linux, D1\n---\n\n这里讲解怎样自己添加驱动\n\n<!-- more -->\n\n[原文链接](https://bbs.aw-ol.com/topic/994/d1-licheerv-dock-%E7%A7%BB%E6%A4%8Drtl8723ds%E9%A9%B1%E5%8A%A8)\n\n手动焊接RTL8723DS之后，现在开始移植驱动程序。\n\n先获取源码：https://github.com/lwfinger/rtl8723ds\n\n下载完成后，把驱动文件复制到 tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds 里，没有rtl8723ds文件夹记得新建一个。\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\Makefile，加一行 \nobj-$(CONFIG_RTL8723DS) += rtl8723ds/\n\n![](./assets/rtl8723ds.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\Kconfig，加一行 \nsource \"drivers/net/wireless/rtl8723ds/Kconfig\"\n\n![](./assets/Kconfig.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds\\os_dep\\linux\\os_intfs.c；\n加一行\nMODULE_IMPORT_NS(VFS_internal_I_am_really_a_filesystem_and_am_NOT_a_driver);\n\n![](./assets/os_intfs.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds\\os_dep\\linux\\rtw_cfgvendor.c\n在每一行.policy = VENDOR_CMD_RAW_DATA, 下面加上 .maxattr = 1,\n\n![](./assets/rtw_cfgvendor.png)\n\n修改tina-d1-open\\target\\allwinner\\d1-lichee_rv_dock\\modules.mk，增加以下内容：\n\n![](./assets/modules.png)\n\n（其中的d1-lichee_rv_dock 是我的板级配置，请选择自己的板级配置比如d1-nezha，如下图）\n\n![](./assets/borad_config.png)\n\n进入内核配置，勾选Realtek 8723D SDIO or SPI WiFi为Module（ < M > 不是 < * > ）\n\n```menuconfig\nmake kernel_menuconfig\n\nDevice Drivers ->\n     Network device support -> \n           Wireless LAN -> \n                  <M>   Realtek 8723D SDIO or SPI WiFi\n```\n\n进入Tina配置，勾选相关驱动\n\n```bash\nmake menuconfig\n\nFirmware  ->\n     <*> r8723ds-firmware.............................. RealTek RTL8723DS firmware\n\nKernel modules -> \n     Wireless Drivers  ->\n        <*> kmod-net-rtl8723ds........................... RTL8723DS support (staging)\n```\n\n保存，编译，打包\n\n```bash\nmake -j8\npack\n```\n\n烧录后就能看到\n\n![](./assets/apperance.jpg)"}, "/news/Lichee/RV/run_nonos_program/nonos_run.html": {"title": "在D1上使用裸机程序", "content": "---\ntitle: 在D1上使用裸机程序\nkeywords: RV, D1, 裸机\ndate: 2022-04-29\ntags: MaixPy,MaixPy3\n---\n近日一国内小哥实现了一种在D1上运行裸机程序的方法。让我们一起来看一下吧\n\n<!-- more -->\n\nGithub仓库地址在这里：https://github.com/Ouyancheng/FlatHeadBro\n\n相关使用方法在仓库的 readme 写的很详细了。\n\n大概就是用先编译出一个 启动固件，烧录到SD卡启动板子后可以看到串口有相关的信息打印出来。\n\n接着只需要使用 python 将想要运行的程序通过串口传送到开发板上面即可。"}, "/news/others/v831_resnet18/v831_resnet18.html": {"title": "在V831上（awnn）跑 pytorch resnet18 模型", "content": "---\ntitle: 在V831上（awnn）跑 pytorch resnet18 模型\nkeywords: V831,awnn,resnet18\ndate: 2022-06-13\ntags: V831,awnn,resnet18\n---\n\n在V831上（awnn）跑 pytorch resnet18 模型，和模型转换方法\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/358\n\n原文时间：2021.04.10， 搬运有改动\n\n- 可以参考一下\n\n## 直接使用 Pytorch hub 与模型训练\n\n此处省略模型定义和训练过程，仅使用 pytorch hub 的 resnet18 预训练模型进行简单介绍\n\nhttps://pytorch.org/hub/pytorch_vision_resnet/\n\n## 在 PC 端测试模型推理\n\n根据上面链接的使用说明，使用下面代码可以运行模型\n\n其中，label 下载：https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\n```python\nimport os\nimport torch\nfrom torchsummary import summary\n\n## model\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\nmodel.eval()\ninput_shape = (3, 224, 224)\nsummary(model, input_shape, device=\"cpu\")\n\n## test image\nfilename = \"out/dog.jpg\"\nif not os.path.exists(filename):\n    if not os.path.exists(\"out\"):\n        os.makedirs(\"out\")\n    import urllib\n    url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename)\n    try: urllib.URLopener().retrieve(url, filename)\n    except: urllib.request.urlretrieve(url, filename)\nprint(\"test image:\", filename)\n\n## preparing input data\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\ninput_image = Image.open(filename)\n\n# input_image.show()\npreprocess = transforms.Compose([\n    transforms.Resize(max(input_shape[1:3])),\n    transforms.CenterCrop(input_shape[1:3]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\nprint(\"input data max value: {}, min value: {}\".format(torch.max(input_tensor), torch.min(input_tensor)))\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n\n## forward model\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to('cuda')\n    model.to('cuda')\nwith torch.no_grad():\n    output = model(input_batch)\n\n## result    \n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n# print(output[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nmax_1000 = torch.nn.functional.softmax(output[0], dim=0)\nmax_idx = int(torch.argmax(max_1000))\nwith open(\"imagenet_classes.txt\") as f:\n    labels = f.read().split(\"\\n\")\nprint(\"result: idx:{}, name:{}\".format(max_idx, labels[max_idx]))\n```\n\n运行结果如下：\n\n```python\nUsing cache found in /home/neucrack/.cache/torch/hub/pytorch_vision_v0.6.0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n       BasicBlock-11           [-1, 64, 56, 56]               0\n           Conv2d-12           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-13           [-1, 64, 56, 56]             128\n             ReLU-14           [-1, 64, 56, 56]               0\n           Conv2d-15           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-16           [-1, 64, 56, 56]             128\n             ReLU-17           [-1, 64, 56, 56]               0\n       BasicBlock-18           [-1, 64, 56, 56]               0\n           Conv2d-19          [-1, 128, 28, 28]          73,728\n      BatchNorm2d-20          [-1, 128, 28, 28]             256\n             ReLU-21          [-1, 128, 28, 28]               0\n           Conv2d-22          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-23          [-1, 128, 28, 28]             256\n           Conv2d-24          [-1, 128, 28, 28]           8,192\n      BatchNorm2d-25          [-1, 128, 28, 28]             256\n             ReLU-26          [-1, 128, 28, 28]               0\n       BasicBlock-27          [-1, 128, 28, 28]               0\n           Conv2d-28          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-29          [-1, 128, 28, 28]             256\n             ReLU-30          [-1, 128, 28, 28]               0\n           Conv2d-31          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-32          [-1, 128, 28, 28]             256\n             ReLU-33          [-1, 128, 28, 28]               0\n       BasicBlock-34          [-1, 128, 28, 28]               0\n           Conv2d-35          [-1, 256, 14, 14]         294,912\n      BatchNorm2d-36          [-1, 256, 14, 14]             512\n             ReLU-37          [-1, 256, 14, 14]               0\n           Conv2d-38          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-39          [-1, 256, 14, 14]             512\n           Conv2d-40          [-1, 256, 14, 14]          32,768\n      BatchNorm2d-41          [-1, 256, 14, 14]             512\n             ReLU-42          [-1, 256, 14, 14]               0\n       BasicBlock-43          [-1, 256, 14, 14]               0\n           Conv2d-44          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-45          [-1, 256, 14, 14]             512\n             ReLU-46          [-1, 256, 14, 14]               0\n           Conv2d-47          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-48          [-1, 256, 14, 14]             512\n             ReLU-49          [-1, 256, 14, 14]               0\n       BasicBlock-50          [-1, 256, 14, 14]               0\n           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n             ReLU-53            [-1, 512, 7, 7]               0\n           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n           Conv2d-56            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n             ReLU-58            [-1, 512, 7, 7]               0\n       BasicBlock-59            [-1, 512, 7, 7]               0\n           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n             ReLU-62            [-1, 512, 7, 7]               0\n           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n             ReLU-65            [-1, 512, 7, 7]               0\n       BasicBlock-66            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                 [-1, 1000]         513,000\n================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 62.79\nParams size (MB): 44.59\nEstimated Total Size (MB): 107.96\n----------------------------------------------------------------\nout/dog.jpg\ntensor(2.6400) tensor(-2.1008)\nidx:258, name:Samoyed, Samoyede\n```\n\n可以看到模型有 11,689,512 个参数，差不多 11MiB 左右，这个大小也几乎是实际在 831 上运行的模型大小了\n\n## 将模型转换为 V831 能使用的模型文件\n\n转换过程如下：\n\n使用 Pytorch 将模型导出为 onnx 模型， 得到 onnx 文件\n\n```python\ndef torch_to_onnx(net, input_shape, out_name=\"out/model.onnx\", input_names=[\"input0\"], output_names=[\"output0\"], device=\"cpu\"):\n    batch_size = 1\n    if len(input_shape) == 3:\n        x = torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], dtype=torch.float32, requires_grad=True).to(device)\n    elif len(input_shape) == 1:\n        x = torch.randn(batch_size, input_shape[0], dtype=torch.float32, requires_grad=False).to(device)\n    else:\n        raise Exception(\"not support input shape\")\n    print(\"input shape:\", x.shape)\n    # torch.onnx._export(net, x, \"out/conv0.onnx\", export_params=True)\n    torch.onnx.export(net, x, out_name, export_params=True, input_names = input_names, output_names=output_names)\nonnx_out=\"out/resnet_1000.onnx\"\nncnn_out_param = \"out/resnet_1000.param\"\nncnn_out_bin = \"out/resnet_1000.bin\"\ninput_img = filename\ntorch_to_onnx(model, input_shape, onnx_out, device=\"cuda:0\")\n```\n\n如果你不是使用 pytorch 转换的, 而是使用了现成的 ncnn 模型, 不知道输出层的名字, 可以在 https://netron.app/ 打开模型查看输出层的名字\n\n使用 onnx2ncnn 工具将 onnx 转成 ncnn 模型，得到一个 .param 文件和一个 .bin 文件\n按照 ncnn 项目的编译说明编译，在 build/tools/onnx 目录下得到 onnx2ncnn 可执行文件\n\n```python\ndef onnx_to_ncnn(input_shape, onnx=\"out/model.onnx\", ncnn_param=\"out/conv0.param\", ncnn_bin = \"out/conv0.bin\"):\n    import os\n    # onnx2ncnn tool compiled from ncnn/tools/onnx, and in the buld dir\n    cmd = f\"onnx2ncnn {onnx} {ncnn_param} {ncnn_bin}\"\n    os.system(cmd)\n    with open(ncnn_param) as f:\n        content = f.read().split(\"\\n\")\n        if len(input_shape) == 1:\n            content[2] += \" 0={}\".format(input_shape[0])\n        else:\n            content[2] += \" 0={} 1={} 2={}\".format(input_shape[2], input_shape[1], input_shape[0])\n        content = \"\\n\".join(content)\n    with open(ncnn_param, \"w\") as f:\n        f.write(content)\nonnx_to_ncnn(input_shape, onnx=onnx_out, ncnn_param=ncnn_out_param, ncnn_bin=ncnn_out_bin)\n\n```\n## 使用全志提供的awnn工具将ncnn模型进行量化到int8模型\n\n在 maix.sipeed.com 模型转换 将 ncnn 模型转换为 awnn 支持的 int8 模型 （网页在线转换很方便人为操作，另一个方面因为全志要求不开放 awnn 所以暂时只能这样做）\n\n阅读转换说明，可以获得更多详细的转换说明\n\n![](./assets/convert.png)\n\n这里有几组参数：\n\n- 均值 和 归一化因子： 在 pytorch 中一般是 `(输入值 - mean ) / std`, awnn 对输入的处理是 `(输入值 - mean ) * norm`, 总之，让你训练的时候的输入到第一层网络的值范围和给 awnn 量化工具经过 `(输入值 - mean ) * norm` 计算后的值范围一致既可。 比如这里打印了实际数据的输入范围是 [-2.1008, 2.6400]， 是代码中 preprocess 对象处理后得到的，即 `x = (x - mean) / std ==> (0-0.485)/0.229 = -2.1179`, 到 awnn 就是 `x = (x - mean_2*255) * (1 / std * 255)` 即 `mean2 = mean * 255`, `norm = 1/(std * 255)`, 更多可以看[这里](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-produce-wrong-result#pre-process)。\n所以我们这里可以设置 均值为 `0.485 * 255 = 123.675`， 设置 归一化因子为 `1/ (0.229 * 255) = 0.017125`， 另外两个通道同理。但是目前 awnn 只能支持三个通道值一样。。。所以填 `123.675, 123.675, 123.675，0.017125, 0.017125, 0.017125` 即可，因为这里用了 pytorch hub 的预训练的参数，就这样吧， 如果自己训练，可以好好设置一下\n\n- 图片分辨率（问不是图片怎么办？貌似 awnn 暂时之考虑到了图片。。）\n\n- RGB 格式： 如果训练输入的图片是 RGB 就选 RGB\n- 量化图片， 选择一些和输入尺寸相同的图片，可以从测试集中拿一些，不一定要图片非常多，但尽量覆盖全场景（摊手\n\n自己写的其它模型转换如果失败，多半是啥算子不支持，上图框出的地方查看所支持的算子，比如现在的版本view、 flatten、reshape 都不支持所以写模型要相当小心，后面的版本会支持 flatten reshape 等 CPU 算子\n\n如果不出意外， 终于得到了量化好的 awnn 能使用的模型， *.param 和 *.bin\n\n## 使用模型，在v831上推理\n\n可以使用 python 或者 C 写代码，以下两种方式\n\n### MaixPy3\n\npython 请看 [MaixPy3](https://wiki.sipeed.com/soft/maixpy3/zh/)\n\n不想看文档的话，就是在系统开机使用的基础上， 更新 MaixPy3 就可以了：\n\n```bash\npip install --upgrade maixpy3\n```\n\n然后在终端使用 python 运行脚本（可能需要根据你的文件名参数什么的改一下代码）：\n\nhttps://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/load_forward_camera.py\n\nlabel 在这里： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/classes_label.py\n\n```python\nfrom maix import nn\nfrom PIL import Image, ImageDraw\nfrom maix import camera, display\n\ntest_jpg = \"/root/test_input/input.jpg\"\nmodel = {\n    \"param\": \"/root/models/resnet_awnn.param\",\n    \"bin\": \"/root/models/resnet_awnn.bin\"\n}\n\ncamera.config(size=(224, 224))\n\noptions = {\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": (224, 224, 3)\n    },\n    \"outputs\": {\n        \"output0\": (1, 1, 1000)\n    },\n    \"first_layer_conv_no_pad\": False,\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196],\n}\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\n\nprint(\"-- read image\")\nimg = Image.open(test_jpg)\nprint(\"-- read image ok\")\nprint(\"-- forward model with image as input\")\nout = m.forward(img, quantize=True)\nprint(\"-- read image ok\")\nprint(\"-- out:\", out.shape)\nout = nn.F.softmax(out)\nprint(out.max(), out.argmax())\n\nfrom classes_label import labels\nwhile 1:\n    img = camera.capture()\n    if not img:\n        time.sleep(0.02)\n        continue\n    out = m.forward(img, quantize=True)\n    out = nn.F.softmax(out)\n    msg = \"{:.2f}: {}\".format(out.max(), labels[out.argmax()])\n    print(msg)\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 0), msg, fill=(255, 0, 0))\n    display.show(img)\n```\n\n### C 语言 SDK,libmiax\n\n按照 https://github.com/sipeed/libmaix 的说明克隆仓库，并编译 https://github.com/sipeed/libmaix/tree/master/examples/nn_resnet\n\n上传编译成功后dist目录下的所有内容到 v831, 然后执行./start_app.sh即可"}, "/news/others/Python_call_so.html": {"title": "Python3调用c/cpp的方法", "content": "---\ntitle: Python3调用c/cpp的方法\nkeywords: python, c, cpp,\ndesc: python调用so\ndate: 2022-03-31\ntags: python, c, cpp\n---\n\n<!-- more -->\n\n原文链接：https://blog.csdn.net/springlustre/article/details/101177282\n作者：[springlustre](https://blog.csdn.net/springlustre?type=blog)\n有改动，仅供参考\n\npython中使用 ctypes 模块可以在python中直接调用C/C++。\n首先要将C/C++编译成动态库（.so)，然后python中调用即可。\n\n特别注意在调用C++函数需要在函数声明时，加入前缀 extern \"C\" ，这是因为C++支持函数重载功能，在编译时会改变函数名。在函数声明时，前缀extern \"C\"可以确保按C的方式进行编译。\n\n值得注意的是，一定要有函数输入输出类型的声明，int型不用转换，float和double类型需要进行转换；\nctypes中的变量类型与C中对应如下：\n\n| ctypes数据类型 | C数据类型     |\n| -------------- | ------------- |\n| c_char         | char          |\n| c_short        | short         |\n| c_int          | int           |\n| c_long         | long          |\n| c_float        | float         |\n| c_double       | double        |\n| c_void_p       | void          |\n| c_uint8        | unsigned char |\n\n使用方法：\n- 编写c++代码\n\n```cpp\n#include <iostream>\n#include <string>\n#include <cstdlib>\n#include <vector>\n#include <stdio.h>\n\n\nclass Test{\n    private:\n        double _calculate(int a, double b);\n    public:\n        double calculate(int a, double b, char c[], int * d, double * e, char ** f);\n};\n\ndouble Test::_calculate(int a, double b){\n    double res = a+b;\n    std::cout<<\"res: \"<<res<<std::endl;\n    return res;\n}\n\ndouble Test::calculate(int a, double b, char c[], int * d, double * e, char ** f){\n    std::cout<<\"a: \"<<a<<std::endl;\n    std::cout<<\"b: \"<<b<<std::endl;\n    std::cout<<\"c: \"<<c<<std::endl;\n    std::cout<<\"d: \"<<d[0]<<d[1]<<std::endl;\n    std::cout<<\"e: \"<<e[0]<<e[1]<<std::endl;\n    std::cout<<\"f: \"<<f[0]<<f[1]<<std::endl;\n    return this->_calculate(a, b);\n}\n\n\n// 封装C接口\nextern \"C\"{\n// 创建对象\n    Test* test_new(){\n        return new Test;\n    }\n    double my_calculate(Test* t, int a, double b, char c[], int * d, double * e, char ** f){\n        return t->calculate(a, b,c,d,e,f);\n    }\n}\n\n```\n- 将上面的代码编译成so文件\n\n> g++ -shared -Wl,-soname,test -o test.so -fPIC test.cpp\n\n- 使用python调用so文件\n\n```python\n# -*- coding: utf-8 -*-\nimport ctypes\n# 指定动态链接库\nlib = ctypes.cdll.LoadLibrary('./test.so')\n#需要指定返回值的类型，默认是int\nlib.my_calculate.restype = ctypes.c_double\n\nclass Test(object):\n    def __init__(self):\n        # 动态链接对象\n        self.obj = lib.test_new()\n\n    def calculate(self, a, b,c,d,e,f):\n        res = lib.my_calculate(self.obj, a, b,c,d,e,f)\n        return res\n\n#将python类型转换成c类型，支持int, float,string的变量和数组的转换\ndef convert_type(input):\n    ctypes_map = {int:ctypes.c_int,\n              float:ctypes.c_double,\n              str:ctypes.c_char_p\n              }\n    input_type = type(input)\n    if input_type is list:\n        length = len(input)\n        if length==0:\n            print(\"convert type failed...input is \"+input)\n            return null\n        else:\n            arr = (ctypes_map[type(input[0])] * length)()\n            for i in range(length):\n                arr[i] = bytes(input[i],encoding=\"utf-8\") if (type(input[0]) is str) else input[i]\n            return arr\n    else:\n        if input_type in ctypes_map:\n            return ctypes_map[input_type](bytes(input,encoding=\"utf-8\") if type(input) is str else input)\n        else:\n            print(\"convert type failed...input is \"+input)\n            return null\n\nif __name__ == '__main__':\n    t = Test()\n    A1\t= 123;\n    A2\t= 0.789;\n    A3\t= \"C789\";\n    A4\t= [456,789];\n    A5\t= [0.123,0.456];\n    A6\t= [\"A123\", \"B456\"];\n    print(t.calculate(convert_type(A1), convert_type(A2), convert_type(A3),convert_type(A4),convert_type(A5),convert_type(A6)))\n```"}, "/news/others/color_introduction/color_introduction.html": {"title": "常见的图像颜色空间解释", "content": "---\ntitle: 常见的图像颜色空间解释\nkeywords: Color, 色彩空间,RGB,HSV,YUV,LAB,CMYK\ndesc: 色彩空间科普\ndate: 2022-06-11\ntags: Color, 色彩空间\ncover: ./assets/cover.png\n---\n\n常用颜色表示方法： RGB HSV YUV LAB CMYK\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/294\n\n## RGB\n\n红绿蓝 三色，也是大家熟悉光学三原色\n\nRGB 使用加色模式，也就是默认是黑色，三原色相加获得白色， 比如下图`蓝色+绿色=青色(cyan)`，得到`蓝色=青色-绿色`也就是`蓝色=青色+绿色`的互补色， 绿色的互补色就是图中的`M(品红色）magenta`（同理蓝色的互补色是Y黄色）（互补色就是两者相加为白色），所以`蓝色=青色+品红色`\n\n![](./assets/color_add.png)\n\n![](./assets/coordinate_box.png)\n\n上图可以看到灰度图在正方体对角线上，即三个通道（轴）的值相等时，值越大越白\n\n![](./assets/color.png)\n\n一般两种表示方法：\n\n### RGB888(24bit)\n\nRGB三个通道，每个通道分别用8bit长度表示，比如(255, 255, 255), 每个通道取值范围为[0, 255]，或者0xFFFFFF三个字节表示\n\n### RGB565(16bit)\n\nRGB三个通道分别用5bit 6bit 5bit表示，比如(31, 63, 31)，但一般不这样表示，使用两个字节表示，比如0xFFFF, 一般在编写程序时在内存中多使用以下两种布局方式：\n\n- 第一种：\n\n![](./assets/bgr_color.png)\n\n- 第二种：\n\n![](./assets/grgb_color.png)\n\n这两种方式的不同主要是因为 RGB565 共占用 2 个字节， 两个字节的顺序不同造成的，把第二张图从中间分隔成两份，右边移到左边，就变成了第一种的排列方式了\n\nC语言结构体如下：\n```c\n#define COLOR_16_SWAP 1\ntypedef union\n{\n    struct\n    {\n#if COLOR_16_SWAP == 0\n        uint16_t blue : 5;\n        uint16_t green : 6;\n        uint16_t red : 5;\n#else\n        uint16_t green_h : 3;\n        uint16_t red : 5;\n        uint16_t blue : 5;\n        uint16_t green_l : 3;\n#endif\n    } ch;\n    uint16_t full;\n} color16_t;\n```\n\n比如`(1,2,3)` `(R, G, B)`：\n\n使用第一种方式二进制值为 `B00001 000010 00011 ` 即 `B0000 1000 0100 0011`, 十六进制表示为 `0x0843`（注意这里表示方法从左到右是从高位到低位，上面的图从左到右是低位到高位）；\n\n使用第二种方式二进制值为 `B010 00011 00001 000 `即 `B0100 0011 0000 1000`, 十六进制表示为 `0x4308`；\n\n## HSV\n\n相关解释：\n- Hue（色调、色相）\n- Saturation（饱和度、色彩纯净度）\n- Value（明度）\n\n![](./assets/hsv.png)\n\n## HLS\n\nHLS 中的 L 分量为亮度，亮度为100，表示白色，亮度为0，表示黑色；HSV 中的 V 分量为明度，明度为100，表示光谱色，明度为0，表示黑色。\n\n提取白色物体时，使用 HLS 更方便，因为 HSV 中的Hue里没有白色，白色需要由S和V共同决定（S=0, V=100）。而在 HLS 中，白色仅由亮度L一个分量决定。所以检测白色时使用 HSL 颜色空间更准确。\n\n![](./assets/hls.png)\n\n## YUV\n\nY'UV、YUV、YCbCr、YPbPr 几个概念其实是一回事儿。Y’UV、YUV 主要是用在彩色电视中，用于模拟信号表示。YCbCr 是用在数字视频、图像的压缩和传输，如 MPEG、JPEG。今天大家所讲的 YUV 其实就是指 YCbCr。Y 表示亮度（luma），CbCr 表示色度（chroma）。\n\n另外Y’UV在取值上可以使正或者负数，但是Y’CbCr一般是 `16–235` 或者 `0–255`\n\nY’UV 设计的初衷是为了使彩色电视能够兼容黑白电视。对于黑白电视信号，只需要 Y 通道， 在彩色电视则显示 YUV 信息\n\n![](./assets/yuv.png)\n\n![](./assets/y'uv.png)\n\n### 打包格式和采样\n\nYUV 格式通常有两大类:打包(packed)格式和平面(planar)格式， 前者是每个像素为基本单位，一个一个像素的数据连续排列在内存中， 后者则是YUV 分成3个数组（内存块）存放， 另外还有Y和UV分开存放的（比如 YUV420SP（ Semi-Planar， U和V交叉放，即YYYYYYYY…UVUV…） 和 YUV420P（先放U再放V，即YYYYYYYY…UUVV））\n\n人眼的视觉特点是对亮度更铭感，对位置、色彩相对来说不铭感。在视频编码系统中为了降低带宽，可以保存更多的亮度信息(luma)，保存较少的色差信息(chroma)。这叫做 chrominance subsamping, 色度二次采样。原则：在数字图像中，(1) 每一个图形像素都要包含 luma（亮度）值；（2）几个图形像素共用一个 Cb + Cr 值，一般是 2、4、8 个像素。 一般分为以下几种\n\n- YUV444：就是每个 Y 值对应一个 U 和 一个 V 值\n- YUV422: 就是图像的横轴（width方向） 4 个 Y 值公用 2 个 U 和V，纵轴（height方向）4 个 Y 对应了 4 个 U 和 4 个 V\n- YUV420: 就是在 YUV422 的基础上， 纵轴的 U 和 V 数量也减半，每 2 个提供给 4 个 Y 配对使用\n- YUV411: 同理，就是 4 个 Y， 对应横轴和纵轴的 1 个 U 和 V\n\n![](./assets/yuv_pack.png)\n\n`SP`(Semi-Planar) 和 `P` 的说法， 区别就是在内存中的存放顺序不同, 比如\n\nYUV420SP:\n\n![](assets/yuv420sp.jpg)\n\nYUV420P:\n\n![](assets/yuv420p.jpg)\n\n另外，还有 NV12 和 NV21 的区别， 就是在 UV 的存放上顺序不同\n\n- NV12: IOS只有这一种模式。存储顺序是先存Y，再UV交替存储。YYYYUVUVUV\n- NV21: 安卓的模式。存储顺序是先存Y，再存U，再VU交替存储。YYYYVUVUVU\n\nYUV与RGB之间的转换\n\n略\n\n参考代码： https://github.com/latelee/yuv2rgb\n\n## LAB\n\nLab颜色空间中的L分量用于表示像素的亮度，取值范围是[0,100],表示从纯黑到纯白；a表示从红色到绿色的范围，取值范围是[127,-128]；b表示从黄色到蓝色的范围，取值范围是[127,-128]\n\n![](./assets/LAB.png)\n\n## CMY & CMYK\n\n![](./assets/cmyk_1.jpg)\n\n一般用在印刷， 因为人眼看到的物体的颜色是反射光，而不是自发光，一束白光照到物体上，默认反射所有，即白色，人眼实际看到的颜色是用光的颜色减去材料吸收后的颜色,这时涂上黑色的颜料，即吸收了所有白光，所以人眼看到的是黑色。\n利用色料的三原色混色原理，加上黑色油墨，共计四种颜色混合叠加，形成所谓“全彩印刷”。四种标准颜色是：`C：Cyan = 青色`，又称为‘天蓝色’或是‘湛蓝’`M：Magenta = 品红色`，又称为‘洋红色’；`Y：Yellow = 黄色`；`K：blacK=黑色`，虽然有文献解释说这里的K应该是Key Color（定位套版色），但其实是和制版时所用的定位套版观念混淆而有此一说。此处缩写使用最后一个字母K而非开头的B，是为了避免与Blue混淆。CMYK模式是减色模式，相对应的RGB模式是加色模式。\n\n印刷三原色如何得到黑色， 理论配色如下：\n\n> C(100)  +M（100） +Y（100） = 黑色（100，100，100）\n\n可见黑色就是青色、品与黄色之和，但是这三种颜色混成的黑色不够纯，所以印刷学就引进了K(Black)黑色，因为B已经被Blue占用，所以黑色就只好用引文字母黑色的最后一个字母K, 哪么真正印刷的黑色配色如下:\n\n> C(100)  +M（100） +Y（100） + K(100) = 黑色 （100，100，100，100）\n\n或者\n\n> C(0)  +M(0) + Y(0) + K(100) = 黑色(0，0，0，100）\n\n减色模式：前面说的由于物体是吸收了部分光，才呈现出特有的颜色，比如品红的物体，是吸收了它的互补色绿色（看前面的RGB对互补色的描述），也就是白色光减去了绿色才得到的颜色，所以称之为减色模式"}, "/news/others/maixII_connect_udisk.html": {"title": "MaixII 通过 USB OTG 口连接U盘", "content": "---\ntitle: MaixII 通过 USB OTG 口连接U盘\nkeywords: MaixII, U盘\ndate: 2022-06-14\ntags: MaixII, U盘\n---\n\nMaixII的USB口是用来做device连接电脑跑adb的。\n但是有没有方法可以在不跑adb的时候（总不能天天跑adb吧，再说adb也可以网络跑啊）连接一些USB设备玩玩呢。\n\n<!-- more -->\n\n原文链接：https://bbs.sipeed.com/thread/844 有改动\n\n## 摸索过程\n\nMaixII dock 有两个接口，我们要更改 otg 口因此我们使用 UART口 连接电脑来更改板子设置\n\n### 看看在那里定义了 \n\n在 /etc/init.d/ 文件夹里面可以看到有如下的文件\n\n```bash\nroot@sipeed:# ls /etc/init.d\nS00mpp       S10udev      S40network   S52ntpd      log          rc.preboot\nS01audio     S11dev       S41netparam  adbd         network      rcK\nS01logging   S12usb       S50telnet    cron         rc.final     rcS\nS02app       S20urandom   S51dropbear  fontconfig   rc.modules   sysntpd\n```\n\n注意到里面有一个 `S12usb`\n\n使用 `cat /etc/init.d/S12usb` 查看里面内容后发现有一句 \n\n```bash\notg_role=`cat /sys/devices/platform/soc/usbc0/otg_role`\n```\n\n抱着好奇的心态在设备上跑了这句脚本，结果如下所示：\n\n```bash\nroot@sipeed:~# cat /sys/devices/platform/soc/usbc0/otg_role\nusb_device\n```\n\n### 切换为 USB host\n\n再好奇下看这个 /sys/devices/platform/soc/usbc0 目录中都有啥，结果如下：\n\n```bash\nroot@sipeed:~# ls /sys/devices/platform/soc/usbc0\ndriver           hw_scan_debug    of_node          subsystem        usb_device       usb_null\ndriver_override  modalias         otg_role         uevent           usb_host\n```\n\n重点是里面的：`usb_device` `usb_host` `usb_null`\n\n那直接把 `usb_host` echo 到 `/sys/devices/platform/soc/usbc0/otg_role` 中看看啥效果：\n\n```bash\necho \"usb_host\" > /sys/devices/platform/soc/usbc0/otg_role\n```\n\n然后我们使用 `lsusb` 看看都有啥\n\n```bash\nroot@sipeed:~# lsusb\nBus 001 Device 001: ID 1d6b:0002\nBus 002 Device 001: ID 1d6b:0001\n```\n\n哈，USB控制器出来了。\n\n### 连接USB设备\n\n想着设备内识别SD卡，那U盘应该也差不多。插个U盘试下。\n\n```bash\nroot@sipeed:~# lsusb\nBus 001 Device 001: ID 1d6b:0002\nBus 001 Device 002: ID aaaa:8816\nBus 002 Device 001: ID 1d6b:0001\n```\n\n多出来一个设备，在 /dev 目录下看了下果然多出来一个sda：\n\n```bash\nroot@sipeed:/# ls /dev/sda\nsda   sda1  sda2\n```\n\n挂载 U盘 试试：\n\n出现 `Read-only file system` 的话，重烧是最快的解决方法。\n\n```bash\nroot@sipeed:~# mkdir -p /home/usbdisk\nroot@sipeed:~# mount /dev/sda2 /home/usbdisk/\nroot@sipeed:~# df\nFilesystem           1K-blocks      Used Available Use% Mounted on\n/dev/root               256512     88352    162920  35% /\ntmpfs                    29864        12     29852   0% /tmp\nnone                     29796         0     29796   0% /dev\n/dev/mmcblk0p3            2013         1      2013   0% /mnt/cfg\n/dev/mmcblk0p6         2939292     59664   2863244   2% /mnt/UDISK\n/dev/sda4              7926272    405644   7520628   5% /home/usbdisk\n```\n\n挂载成功。\n\n然后，试了下无线网卡、USB串口啥的，基本都没识别出来，估计是驱动没有编译进去吧。"}, "/news/index.html": {"title": "动态", "content": "---\n\ntitle: 动态\nkeywords: teedoc, 博客生成, 静态博客\ndesc: teedoc 静态博客页面生成\nshow_source: false\ndate: true\n\n---\n<div id=\"blog_list\"></div>"}, "/news/MaixPy/reload_python_module.html": {"title": "关于 MicroPython import 指定 flash 或 sd 分区的代码与重载 Python 模块的方法", "content": "---\ntitle: 关于 MicroPython import 指定 flash 或 sd 分区的代码与重载 Python 模块的方法\nkeywords: MicroPython\ndate: 2022-06-09\ntags: MicroPython, reload\n---\n\n如果在 maixpy (micropython) 上同时存在 flash 和 sd 等多个分区挂载 / 目录下，且均存在 boot.py 文件，如何加载指定分区下的 boot.py 模块代码呢？\n\n<!-- more -->\n\n[原文链接](https://www.cnblogs.com/juwan/p/14517375.html) https://www.cnblogs.com/juwan/p/14517375.html\n\n`import boot` 时取决于 os 的 vfs (虚拟文件系统) 对象，它会根据 os.getcwd() 和 os.chdir('/sd') 决定代码寻找的位置（/sd 分区路径），如果是某目录下的代码，则可以使用类似 import test.boot 的结构来查找并 import 它。\n\n示例：\n\n```python\n>>> os.chdir('/flash')\n>>> import boot\nflash: 2942\n>>> os.getcwd()\n'/flash'\n>>> \n```\n\n拓展来讲，如何重载 import boot 后的 boot 模块，管理 sys.modules 模块就行，如下示意。\n\n```python\n>>> import sys\n>>> import boot\n2433\n>>> import boot\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>> sys.modules.pop('boot')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: boot\n>>> os.chdir('/flash')\n>>> import boot\nflash: 2479\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>> os.chdir('/sd')\n>>> import boot\n2488\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>>\n```\n\n即先使用 sys.modules.pop('boot') 后再重新 import 目标 boot 就行"}, "/news/MaixPy/star_maixpy.html": {"title": "MaixPy 上手指南（避坑）之上手篇", "content": "---\ntitle: MaixPy 上手指南（避坑）之上手篇\nkeywords: MaixPy, K210, Python, MicroPython\ndesc: MaixPy 上手指南（避坑） 之上手篇\ndate: 2022-04-01\ntags: MaixPy, K210\n---\n\n> 作者：Ray（Rui）\n\n拿到热乎的 K210 开发板，如何上手使用。我接触了许许多多的小白开发者后，整理出来的资料和路线，希望可以减少你们遇到的问题，可以更加愉快的使用 K210 进行自己的项目开发。\n\n<!-- more -->\n\n## K210 开发板\n\n市面上有很多中关于 K210 的开发板，但是并不是所有的开发板都是可以使用 MaixPy 进行开发的。毕竟不同厂商使用的摄像头、屏幕、引脚上使用，都是由差异性的。目前支持的使用 MaixPy 开发的板子有 Sipeed 家的 [Maix 系列](/hardware/zh/maix/index.html)。\n\n如果是试用别家的开发板，并不能很好的兼容 MaixPy，存在差异性。\n\n## 开箱\n\n拿到开发板，首先需要根据屏幕和摄像头排线上的丝印提示来安装好，即排线上的数字 “1” 和板子卡座边上引脚丝印 “1” 方位对应接上。上电之后，屏幕上会显示出一个红色的界面这是开发板已经正常启动了。（也可能存在部分丝印印反）\n\n### 首先要安装开发环境：\n\n1. [【安装驱动】](/soft/maixpy/zh/get_started/env_install_driver.html) 根据自己使用的开发进行选择需要按安装驱动\n2. [【更新固件】](/soft/maixpy/zh/get_started/upgrade_maixpy_firmware.html) 确保使用的是最新版本的固件，并学习一下每个固件之间的[差异](/soft/maixpy/zh/get_started/upgrade_maixpy_firmware.html#固件命名说明)\n3. [【安装 MaixPy IDE】](/soft/maixpy/zh/get_started/env_maixpyide.html)\n\n如果安装驱动的时候出现安装失败，或者是安装驱动之后，电脑上没有显示 COM 口的，就需要更新一下系统或者是检查一下自己的系统是不是正版的了。因为有部分的盗版系统安装不上驱动，或者是安装驱动之后并显示。或者通过换 USB 口进行连接，也许就可以检测到开发板\n\n### 运行代码检测摄像头\n\n将开发板接到电脑上，打开 MaixPy IDE，运行打开的例程代码，检查自己的屏幕和摄像头是否正确连接上了。如果运行例程代码之后，并没有图像出现来屏幕和 IDE 上时，可能摄像头接反了。\n\n## 开始学习使用\n\n开始使用 K210 之前，一定要学习 Python，如果你连 Python 都不会的，就不要继续往下走，可以快速的过一遍 [Python](/soft/maixpy3/zh/origin/python.html) 的语法和使用，一定要会 Python !一定要会 Python !一定要会 Python !\n\n现在就当你懂 Python 了，这是就可以开始看 MaixPy 文档中的入门指南，进行对于 MaixPy 的使用和 K210 的基本了解。\n\n【更多功能应用】中将有 MaixPy 更多的使用案例和使用方式，一定要确保自己已经对应入门教程中内容已经了解和掌握了再去看，否则你在学习的时候还是会一脸懵逼，不知所云。\n\n## 获取 AI 模型文件\n\n在【更多功能应用】中是有讲述如何运行神经网络模型，也知道怎么去获取示例中的模型文件，但是少了如何获取机器码这个操作，这里详细的讲述一下何如获取机器码。\n\n1. 将 [key_gen.bin](https://dl.sipeed.com/fileList/MaixHub_Tools/key_gen_v1.2.bin) 这个固件通过 Kflash 烧录到开发板上。烧录这个机器码固件之后，开发板是处于一个不能使用的状态，上电屏幕只会变成一个白屏。\n2. 这时将开发板通过 USB 连接到电脑上，利用[【串口连接】](/soft/maixpy/zh/get_started/env_serial_tools.html)中的方式来连接开发板。注：IDE 中的串口终端和 IDE 的连接方式相对独立的，而且串口不能通过多种方式进行连接\n3. 利用串口软件连接上开发板，这时按下开发板上的 reset 的按键，就会出现一串字符在终端窗口上，这就机器码。如果机器码\n\n> 推荐使用 IDE 中的 串口终端进行查看，这个相对别的软件更加适合 K210\n\n机器码是一机一码的一种加密方式，用于模型文件的加密。如果使用别的机器码去加密或者下载以 smodel 为文件后缀的模型文件，开发板是无法使用该模型文件的。"}, "/news/MaixPy/K210_kflash_ISP_download_progress.html": {"title": "K210 kflash ISP 下载程序流程", "content": "---\ntitle: K210 kflash ISP 下载程序流程\nkeywords: K210, kflash, ISP\ndate: 2022-06-09\ntags: K210, kflash\n---\n\nK210详细的的程序下载流程，包括芯片侧和kflash侧\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/312 有改动\n\n## 术语\n\n- ISP: In System Programming, 在系统编程\n\n## kflash 下载流程\n\n> 带 || 开头的是芯片侧的操作\n\n- || 芯片拷贝 `boot rom` (特定的硬件，一次性写入)中的 boot 程序到内存末尾1()，并调用运行这个 boot 程序\n\n- || boot 程序运行后， 从 otp 读取信息来判断是否需要从 top 中读取新的 boot，因为 boot rom 区域是一次性写入的，如果 boot 写出了 bug，可以用 otp 中写入新的 boot 来挽救，相当于芯片出厂有两次写入 boot 的机会。而事实是 k210 确实用上了这个功能\n\n- || 如果需要使用 otp 中的新 boot，则读取到内存末尾，但是需要在现在正在运行的 boot 前面，比如之前的末尾1是倒数16k，那么这个 otp 的 boot 就需要写到之前,比如倒数32k到倒数16k位置，然后跳转执行这个新的 boot\n\n- || boot 程序判断 boot 引脚是否被拉低，没被拉低则进入正常启动模式，读取整个固件到内存，然后启动，否则进入ISP模式\n\n- 上位机打开串口\n\n- 上位机通过串口的 dtr rts 来设置 boot 和 reset 引脚，保持拉低 boot引脚，然后拉低reset引脚再拉高reset引脚， 即让芯片重启的时候保持boot引脚为低电平\n\n- || 芯片boot程序检测，如果boot引脚被拉低了，则进入 ISP 模式(输入boot程序的一部分)\n\n- 上位机向芯片发送握手信号(b'\\xc0\\xc2\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0')，然后等待响应信号\n\n- 默认芯片是115200波特率，如果需要更高波特率，发送修改波特率命令给芯片，芯片程序(boot)修改通信波特率\n\n- 向芯片发送写 ramrum 程序的命令，并将需要 RAMRUM 的程序或者新的 ISP 程序发过去，写到芯片内存的起始地址\n\n注意到，这里没有直接发送要写入到flash的程序过去让boot里面的ISP程序去写入到Flash，而是重新发送了一份ISP程序过去，后面会运行这份新的ISP程序来从串口获取固件并烧录到flash，这可以说是为了更灵活，可以自定义ISP程序，而且ISP程序的大小只要小于前面的 boot 和 otp 的新boot的大小之和就行，boot里面做这些事可能面临boot程序过大或者有bug后期更新的问题。但是代价就是每次下载程序都要花费几秒钟下载新的ISP程序，这样用户每次下载的时间会变长，所以最好的肯定是一次性把boot程序写完美，没有bug。其实也可以在boot的ISP程序中加入写如程序到flash的命令,这样是否使用新的ISP就可选\n\n- 向芯片发送启动 ramrum 命令\n\n- 如果是想在ram中运行程序，到这一步为止即可，程序已经在ram中运行了，否则往下一步\n\n- 接下来就是运行发送过去的新ISP程序了\n\n- 上位机和ISP程序握手\n\n- isp程序默认波特率115200，如果需要更高，这里发送更改命令让isp程序修改串口波特率\n\n- 通过串口发送程序文件到isp程序，isp程序写入到 flash\n\n- 通过串口的 dtr 和 rts 控制芯片的reset和boot引脚来正常启动，不进入 ramrum 模式，而是正常从flash加载程序启动"}, "/news/MaixPy/kmodel_datastruct.html": {"title": "K210 kmodel 模型储存数据结构", "content": "---\ntitle: K210 kmodel 模型储存数据结构\nkeywords: K210, kmodel\ndate: 2022-06-09\ntags: K210, kmodel\n---\n\nK210 kmodel 模型储存结构\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/307 有改动\n\n## K210 kmodel 简介\n\nV3 由 nncase v0.1.0 RC5 转换而来\nV4 由 nncase v0.2.0 从 tflite 转换而来\n\nV4相比于V3, 支持了更多算子, 但是运算速度更慢, 部分算子使用了CPU运算, K210侧也使用了C++编写, 部分库会拉低速度, 如果你需要移植或者优化可以注意这一点\n\n## kmodel V3 数据结构\n| 头                  | 输出层信息                        | 各层的头                                 | 各个层数据内容 |\n| ------------------- | --------------------------------- | ---------------------------------------- | -------------- |\n| kpu_kmodel_header_t | kpu_model_output_t * output_count | kpu_model_layer_header_t * layers_length | layers_body    |\n\n这里有个注意点, kpu_kmodel_header_t 没有8字节对齐, 所以在第一层的数据实际是保存在8字节对齐处, 比如前面所有header 长度为 228 字节, 那么 第一层数据中, 头 kpu_model_conv_layer_argument_t 占用24个字节, 228+24 不是8的整数倍, 所以层数据保存在 228+24+4 处, 所以在 kpu_model_conv_layer_argument_t 中用了 layer_offset 这个来表示层数据相对于模型起始地址的偏移\n\n```c\ntypedef struct\n{\n    uint32_t version;     // 固定  0x00000003, 0x03在低地址\n    uint32_t flags;       // 最低位 为1, 表示8bit模式\n    uint32_t arch;\n    uint32_t layers_length;\n    uint32_t max_start_address;\n    uint32_t main_mem_usage;\n    uint32_t output_count;\n} kpu_kmodel_header_t;\ntypedef struct\n{\n    uint32_t address;\n    uint32_t size;\n} kpu_model_output_t;\ntypedef struct\n{\n    uint32_t type;\n    uint32_t body_size;\n} kpu_model_layer_header_t;\n```\n\n## kmodel V4 数据结构\n| 头                    | 输入                     | 输入形状                    | 输出                      | 常量          | 各层的头                        | 各个层数据内容 |\n| --------------------- | ------------------------ | --------------------------- | ------------------------- | ------------- | ------------------------------- | -------------- |\n| struct modelv4_header | memory_range\\*hdr.inputs | runtime_shape_t\\*hdr.inputs | memory_range\\*hdr.outputs | hdr.constants | hdr.nodes \\* struct node_header | nodes content  |\n\n```c\nstruct modelv4_header\n{\n    uint32_t identifier; // 固定为 KMDL, L在低地址\n    uint32_t version;    // 固定为 0x00000004, 0x04 在低位\n    uint32_t flags;\n    uint32_t target;     // CPU: 0, K210: 1\n    uint32_t constants;  // 多少个 uint_t 类型的常量\n    uint32_t main_mem;   // 主内存, 用于AI, 运行时会先把输入的数据拷贝到这里\n    uint32_t nodes;\n    uint32_t inputs;     // input size\n    uint32_t outputs;    // output size\n    uint32_t reserved0;\n};\nstruct node_header\n{\n    uint32_t opcode;\n    uint32_t size;\n};\nstruct memory_range\n{\n    memory_type_t memory_type;\n    datatype_t datatype;\n    uint32_t start;\n    uint32_t size;\n};  // 16 Bytes\ntypedef enum _datatype\n{\n    dt_float32,\n    dt_uint8\n} datatype_t;\ntypedef enum _memory_type\n{\n    mem_const,\n    mem_main,\n    mem_k210_kpu\n} memory_type_t;\nusing runtime_shape_t = std::array<int, 4>;\n```"}}